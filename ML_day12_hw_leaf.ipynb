{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SLCFLAB/Fintech2022/blob/main/ML_day12_hw_leaf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0112d8da"
      },
      "source": [
        "colab에서 열기: https://colab.research.google.com/github/SLCFLAB/Fintech2022/blob/main/ML_day12_hw_leaf.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcIVo6LSvEC7"
      },
      "source": [
        "# PART 04. 작물 잎 사진으로 질병 분류하기"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터는 다음을 통해 다운받을 수 있습니다: https://drive.google.com/drive/folders/1QswvBejKJrc9tz7nNsxGxJblR2vwo6na"
      ],
      "metadata": {
        "id": "LvcMgiAxT7wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 드라이브에 다운받으셨다면, 코랩과 드라이브를 연동시킬 수 있습니다. 위 사이트에서는 zip파일로 되어있는데, 압축을 풀어주세요. 압축을 풀면 dataset이라는 폴더가 생성되는데, 이를 본인 drive에 올려주세요. colab의 사본을 저장하면 자동으로 'Colab Notebook' 폴더에 저장되기 때문에 저는 'Colab Notebook'에 이 폴더를 올려두었어요. 다른 폴더에 올리셔도 무방합니다!"
      ],
      "metadata": {
        "id": "BCDlKCzhX0tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음 코드는 colab과 google drive를 연동하는 코드입니다. 로그인 후 연동에 대한 동의가 필요합니다."
      ],
      "metadata": {
        "id": "r6MFY7QjbACk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "ayZdlSfVVRzD",
        "outputId": "3950e089-48e6-4ce4-893a-07e26a893d47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset 폴더가 있는 위치로 이동시켜주세요. cd를 통해 이동할 수 있습니다. cd gdrive/MyDrive/폴더위치 를 이용합니다. 현재 위치에 어떤 폴더 혹은 파일이 있는지는 !ls를 이용해 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "7f2PCTStX5Uc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "저는 'Colab Notebooks'에 dataset을 올렸기 때문에 다음 코드를 실행시켰습니다."
      ],
      "metadata": {
        "id": "khHDBViubjew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd gdrive/MyDrive/'Colab Notebooks'"
      ],
      "metadata": {
        "id": "ITXSyKM7VYuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced6317a-2dfc-4cce-df32-35d83754490f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intK4FMDvEDJ"
      },
      "source": [
        "## 데이터 분할"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R_59XxcvEDK"
      },
      "source": [
        "* 데이터 분할을 위한 디렉토리 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6c1hXVOVvEDL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        " \n",
        "original_dataset_dir = './dataset'   \n",
        "classes_list = os.listdir(original_dataset_dir) \n",
        " \n",
        "base_dir = './splitted' #이미 splitted 폴더가 존재하면 돌아가지 않아요\n",
        "os.mkdir(base_dir)\n",
        " \n",
        "train_dir = os.path.join(base_dir, 'train') \n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for cls in classes_list:     \n",
        "    os.mkdir(os.path.join(train_dir, cls))\n",
        "    os.mkdir(os.path.join(validation_dir, cls))\n",
        "    os.mkdir(os.path.join(test_dir, cls))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj89GNe1vEDN"
      },
      "source": [
        "* 데이터 분할과 클래스별 데이터 수 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BUc93YHdvEDO",
        "outputId": "985366d9-a219-41cf-de58-77f9be50ef9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size( Pepper,_bell___healthy ):  853\n",
            "Validation size( Pepper,_bell___healthy ):  284\n",
            "Test size( Pepper,_bell___healthy ):  284\n",
            "Train size( Pepper,_bell___healthy (1) ):  12\n",
            "Validation size( Pepper,_bell___healthy (1) ):  4\n",
            "Test size( Pepper,_bell___healthy (1) ):  4\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        " \n",
        "for cls in classes_list:\n",
        "    path = os.path.join(original_dataset_dir, cls)\n",
        "    fnames = os.listdir(path)\n",
        " \n",
        "    train_size = math.floor(len(fnames) * 0.6)\n",
        "    validation_size = math.floor(len(fnames) * 0.2)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "    \n",
        "    train_fnames = fnames[:train_size]\n",
        "    print(\"Train size(\",cls,\"): \", len(train_fnames))\n",
        "    for fname in train_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(train_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    print(\"Validation size(\",cls,\"): \", len(validation_fnames))\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    test_fnames = fnames[(train_size+validation_size):(validation_size + train_size +test_size)]\n",
        "\n",
        "    print(\"Test size(\",cls,\"): \", len(test_fnames))\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCmL1NkDvEDS"
      },
      "source": [
        "## 베이스라인 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6WCF-1tvEDT"
      },
      "source": [
        "* 베이스라인 모델 학습을 위한 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xjNXkSltvEDU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        " \n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "BATCH_SIZE = 256 \n",
        "EPOCH = 30 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eYsACiRfvEDV"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder \n",
        " \n",
        "transform_base = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()]) \n",
        "train_dataset = ImageFolder(root='./splitted/train', transform=transform_base) \n",
        "val_dataset = ImageFolder(root='./splitted/val', transform=transform_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IAVvS4YRvEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb10a45-884c-4032-a90c-4b4d6e6bc654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOG816k9vEDX"
      },
      "source": [
        "* 베이스라인 모델 설계하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XOlYre5tvEDX"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        " \n",
        "class Net(nn.Module): \n",
        "  \n",
        "    def __init__(self): \n",
        "    \n",
        "        super(Net, self).__init__() \n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) \n",
        "        self.pool = nn.MaxPool2d(2,2)  \n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  \n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)  \n",
        "\n",
        "        self.fc1 = nn.Linear(4096, 512) \n",
        "        self.fc2 = nn.Linear(512, 33) \n",
        "    \n",
        "    def forward(self, x):  \n",
        "    \n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)  \n",
        "        x = self.pool(x) \n",
        "        x = F.dropout(x, p=0.25, training=self.training) \n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x) \n",
        "        x = self.pool(x) \n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = self.conv3(x) \n",
        "        x = F.relu(x) \n",
        "        x = self.pool(x) \n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = x.view(-1, 4096)  \n",
        "        x = self.fc1(x) \n",
        "        x = F.relu(x) \n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.fc2(x) \n",
        "\n",
        "        return F.log_softmax(x, dim=1)  \n",
        "\n",
        "model_base = Net().to(DEVICE)  \n",
        "optimizer = optim.Adam(model_base.parameters(), lr=0.001) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I65t1IrivEDY"
      },
      "source": [
        "* 모델 학습을 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2t_Hk1nWvEDY"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()  \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE) \n",
        "        optimizer.zero_grad() \n",
        "        output = model(data)  \n",
        "        loss = F.cross_entropy(output, target) \n",
        "        loss.backward()  \n",
        "        optimizer.step()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHQg7Y5hvEDZ"
      },
      "source": [
        "* 모델 평가를 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-P-4yGp1vEDa"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()  \n",
        "    test_loss = 0 \n",
        "    correct = 0   \n",
        "    \n",
        "    with torch.no_grad(): \n",
        "        for data, target in test_loader:  \n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)  \n",
        "            output = model(data) \n",
        "            \n",
        "            test_loss += F.cross_entropy(output,target, reduction='sum').item() \n",
        " \n",
        "            \n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() \n",
        "   \n",
        "    test_loss /= len(test_loader.dataset) \n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) \n",
        "    return test_loss, test_accuracy  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAdmpf7IvEDa"
      },
      "source": [
        "* 모델 학습을 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo1urbeovEDb",
        "outputId": "61bd2505-5708-41aa-85d0-b7edd5f06410",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- epoch 1 ----------------\n",
            "train Loss: 0.1715, Accuracy: 98.61%\n",
            "val Loss: 0.1591, Accuracy: 98.61%\n",
            "Completed in 0m 23s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 0.2232, Accuracy: 98.61%\n",
            "val Loss: 0.2079, Accuracy: 98.61%\n",
            "Completed in 0m 17s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 0.1034, Accuracy: 98.61%\n",
            "val Loss: 0.0965, Accuracy: 98.61%\n",
            "Completed in 0m 18s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 0.1192, Accuracy: 98.61%\n",
            "val Loss: 0.1174, Accuracy: 98.61%\n",
            "Completed in 0m 17s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import copy\n",
        " \n",
        "def train_baseline(model ,train_loader, val_loader, optimizer, num_epochs = 30):\n",
        "    best_acc = 0.0  \n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) \n",
        " \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        since = time.time()  \n",
        "        train(model, train_loader, optimizer)\n",
        "        train_loss, train_acc = evaluate(model, train_loader) \n",
        "        val_loss, val_acc = evaluate(model, val_loader)\n",
        "        \n",
        "        if val_acc > best_acc: \n",
        "            best_acc = val_acc \n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        time_elapsed = time.time() - since \n",
        "        print('-------------- epoch {} ----------------'.format(epoch))\n",
        "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))   \n",
        "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) \n",
        "    model.load_state_dict(best_model_wts)  \n",
        "    return model\n",
        " \n",
        "\n",
        "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)  \t #(16)\n",
        "torch.save(base,'baseline.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyWed36kvEDc"
      },
      "source": [
        "## Transfer Learning 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SLh5SNBvEDc"
      },
      "source": [
        "* Transfer Learning을 위한 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1rGUW7WvEDd"
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([transforms.Resize([64,64]), \n",
        "        transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(),  \n",
        "        transforms.RandomCrop(52), transforms.ToTensor(), \n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]),\n",
        "    \n",
        "    'val': transforms.Compose([transforms.Resize([64,64]),  \n",
        "        transforms.RandomCrop(52), transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEZdkaaJvEDd"
      },
      "outputs": [],
      "source": [
        "data_dir = './splitted' \n",
        "image_datasets = {x: ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train', 'val']} \n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']} \n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "class_names = image_datasets['train'].classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYZMASiMvEDe"
      },
      "source": [
        "* Pre-Trained Model 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6KcFHfuvEDe"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        " \n",
        "resnet = models.resnet50(pretrained=True)  \n",
        "num_ftrs = resnet.fc.in_features   \n",
        "resnet.fc = nn.Linear(num_ftrs, 33) \n",
        "resnet = resnet.to(DEVICE)\n",
        " \n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)\n",
        " \n",
        "from torch.optim import lr_scheduler\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcg9aGmwvEDe"
      },
      "source": [
        "* Pre-Trained Model의 일부 Layer Freeze하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGH6rYDrvEDe"
      },
      "outputs": [],
      "source": [
        "ct = 0 \n",
        "for child in resnet.children():  \n",
        "    ct += 1  \n",
        "    if ct < 6: \n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_BY_iukvEDf"
      },
      "source": [
        "* Transfer Learning 모델 학습과 검증을 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkguId1vvEDf"
      },
      "outputs": [],
      "source": [
        "def train_resnet(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())  \n",
        "    best_acc = 0.0  \n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
        "        since = time.time()                                     \n",
        "        for phase in ['train', 'val']: \n",
        "            if phase == 'train': \n",
        "                model.train() \n",
        "            else:\n",
        "                model.eval()     \n",
        " \n",
        "            running_loss = 0.0  \n",
        "            running_corrects = 0  \n",
        " \n",
        "            \n",
        "            for inputs, labels in dataloaders[phase]: \n",
        "                inputs = inputs.to(DEVICE)  \n",
        "                labels = labels.to(DEVICE)  \n",
        "                \n",
        "                optimizer.zero_grad() \n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'):  \n",
        "                    outputs = model(inputs)  \n",
        "                    _, preds = torch.max(outputs, 1) \n",
        "                    loss = criterion(outputs, labels)  \n",
        "    \n",
        "                    if phase == 'train':   \n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        " \n",
        "                running_loss += loss.item() * inputs.size(0)  \n",
        "                running_corrects += torch.sum(preds == labels.data)  \n",
        "            if phase == 'train':  \n",
        "                scheduler.step()\n",
        " \n",
        "            epoch_loss = running_loss/dataset_sizes[phase]  \n",
        "            epoch_acc = running_corrects.double()/dataset_sizes[phase]  \n",
        " \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) \n",
        " \n",
        "          \n",
        "            if phase == 'val' and epoch_acc > best_acc: \n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        " \n",
        "        time_elapsed = time.time() - since  \n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        " \n",
        "    model.load_state_dict(best_model_wts) \n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXuKoCT2vEDg"
      },
      "source": [
        "* 모델 학습을 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFKC4qjEvEDg"
      },
      "outputs": [],
      "source": [
        "model_resnet50 = train_resnet(resnet, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH) \n",
        "\n",
        "torch.save(model_resnet50, 'resnet50.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoIWKrfWvEDh"
      },
      "source": [
        "## 모델 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2gapGbwvEDh"
      },
      "source": [
        "* 베이스라인 모델 평가를 위한 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJf4FbgxvEDm"
      },
      "outputs": [],
      "source": [
        "transform_base = transforms.Compose([transforms.Resize([64,64]),transforms.ToTensor()])\n",
        "test_base = ImageFolder(root='./splitted/test',transform=transform_base)  \n",
        "test_loader_base = torch.utils.data.DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IpQegkKvEDn"
      },
      "source": [
        "* Transfer Learning모델 평가를 위한 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t37T4ElovEDn"
      },
      "outputs": [],
      "source": [
        "transform_resNet = transforms.Compose([\n",
        "        transforms.Resize([64,64]),  \n",
        "        transforms.RandomCrop(52),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "    ])\n",
        "    \n",
        "test_resNet = ImageFolder(root='./splitted/test', transform=transform_resNet) \n",
        "test_loader_resNet = torch.utils.data.DataLoader(test_resNet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ox9NFZBvEDq"
      },
      "source": [
        "* 베이스라인 모델 성능 평가하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pNgLu8mvEDq"
      },
      "outputs": [],
      "source": [
        "baseline=torch.load('baseline.pt') \n",
        "baseline.eval()  \n",
        "test_loss, test_accuracy = evaluate(baseline, test_loader_base)\n",
        "\n",
        "print('baseline test acc:  ', test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDkXcLjAvEDr"
      },
      "source": [
        "* Transfer Learning 모델 성능 평가하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26ttIJzxvEDr"
      },
      "outputs": [],
      "source": [
        "resnet50=torch.load('resnet50.pt') \n",
        "resnet50.eval()  \n",
        "test_loss, test_accuracy = evaluate(resnet50, test_loader_resNet)\n",
        "\n",
        "print('ResNet test acc:  ', test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cmrFKrOmc2h9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "ML_day12_hw_leaf",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}