{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SLCFLAB/Fintech2022/blob/main/ML_day12_hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
      {
   "cell_type": "markdown",
   "id": "0112d8da",
   "metadata": {},
   "source": [
    "colab에서 열기: https://colab.research.google.com/github/SLCFLAB/Fintech2022/blob/main/ML_day12_hw2.ipynb"
   ]
  },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcIVo6LSvEC7"
      },
      "source": [
        "# PART 04. 작물 잎 사진으로 질병 분류하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intK4FMDvEDJ"
      },
      "source": [
        "## 데이터 분할"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R_59XxcvEDK"
      },
      "source": [
        "* 데이터 분할을 위한 디렉토리 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c1hXVOVvEDL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        " \n",
        "original_dataset_dir = './dataset'   \n",
        "classes_list = os.listdir(original_dataset_dir) \n",
        " \n",
        "base_dir = './splitted' \n",
        "os.mkdir(base_dir)\n",
        " \n",
        "train_dir = os.path.join(base_dir, 'train') \n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for cls in classes_list:     \n",
        "    os.mkdir(os.path.join(train_dir, cls))\n",
        "    os.mkdir(os.path.join(validation_dir, cls))\n",
        "    os.mkdir(os.path.join(test_dir, cls))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj89GNe1vEDN"
      },
      "source": [
        "* 데이터 분할과 클래스별 데이터 수 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUc93YHdvEDO",
        "outputId": "5225ed95-bc86-47e3-e1dd-6d0539e96b0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size( Apple___Apple_scab ):  378\n",
            "Validation size( Apple___Apple_scab ):  126\n",
            "Test size( Apple___Apple_scab ):  126\n",
            "Train size( Apple___Black_rot ):  372\n",
            "Validation size( Apple___Black_rot ):  124\n",
            "Test size( Apple___Black_rot ):  124\n",
            "Train size( Apple___Cedar_apple_rust ):  165\n",
            "Validation size( Apple___Cedar_apple_rust ):  55\n",
            "Test size( Apple___Cedar_apple_rust ):  55\n",
            "Train size( Apple___healthy ):  987\n",
            "Validation size( Apple___healthy ):  329\n",
            "Test size( Apple___healthy ):  329\n",
            "Train size( Cherry___healthy ):  512\n",
            "Validation size( Cherry___healthy ):  170\n",
            "Test size( Cherry___healthy ):  170\n",
            "Train size( Cherry___Powdery_mildew ):  631\n",
            "Validation size( Cherry___Powdery_mildew ):  210\n",
            "Test size( Cherry___Powdery_mildew ):  210\n",
            "Train size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  307\n",
            "Validation size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
            "Test size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
            "Train size( Corn___Common_rust ):  715\n",
            "Validation size( Corn___Common_rust ):  238\n",
            "Test size( Corn___Common_rust ):  238\n",
            "Train size( Corn___healthy ):  697\n",
            "Validation size( Corn___healthy ):  232\n",
            "Test size( Corn___healthy ):  232\n",
            "Train size( Corn___Northern_Leaf_Blight ):  591\n",
            "Validation size( Corn___Northern_Leaf_Blight ):  197\n",
            "Test size( Corn___Northern_Leaf_Blight ):  197\n",
            "Train size( Grape___Black_rot ):  708\n",
            "Validation size( Grape___Black_rot ):  236\n",
            "Test size( Grape___Black_rot ):  236\n",
            "Train size( Grape___Esca_(Black_Measles) ):  829\n",
            "Validation size( Grape___Esca_(Black_Measles) ):  276\n",
            "Test size( Grape___Esca_(Black_Measles) ):  276\n",
            "Train size( Grape___healthy ):  253\n",
            "Validation size( Grape___healthy ):  84\n",
            "Test size( Grape___healthy ):  84\n",
            "Train size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  645\n",
            "Validation size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
            "Test size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
            "Train size( Peach___Bacterial_spot ):  1378\n",
            "Validation size( Peach___Bacterial_spot ):  459\n",
            "Test size( Peach___Bacterial_spot ):  459\n",
            "Train size( Peach___healthy ):  216\n",
            "Validation size( Peach___healthy ):  72\n",
            "Test size( Peach___healthy ):  72\n",
            "Train size( Pepper,_bell___Bacterial_spot ):  598\n",
            "Validation size( Pepper,_bell___Bacterial_spot ):  199\n",
            "Test size( Pepper,_bell___Bacterial_spot ):  199\n",
            "Train size( Pepper,_bell___healthy ):  886\n",
            "Validation size( Pepper,_bell___healthy ):  295\n",
            "Test size( Pepper,_bell___healthy ):  295\n",
            "Train size( Potato___Early_blight ):  600\n",
            "Validation size( Potato___Early_blight ):  200\n",
            "Test size( Potato___Early_blight ):  200\n",
            "Train size( Potato___healthy ):  91\n",
            "Validation size( Potato___healthy ):  30\n",
            "Test size( Potato___healthy ):  30\n",
            "Train size( Potato___Late_blight ):  600\n",
            "Validation size( Potato___Late_blight ):  200\n",
            "Test size( Potato___Late_blight ):  200\n",
            "Train size( Strawberry___healthy ):  273\n",
            "Validation size( Strawberry___healthy ):  91\n",
            "Test size( Strawberry___healthy ):  91\n",
            "Train size( Strawberry___Leaf_scorch ):  665\n",
            "Validation size( Strawberry___Leaf_scorch ):  221\n",
            "Test size( Strawberry___Leaf_scorch ):  221\n",
            "Train size( Tomato___Bacterial_spot ):  1276\n",
            "Validation size( Tomato___Bacterial_spot ):  425\n",
            "Test size( Tomato___Bacterial_spot ):  425\n",
            "Train size( Tomato___Early_blight ):  600\n",
            "Validation size( Tomato___Early_blight ):  200\n",
            "Test size( Tomato___Early_blight ):  200\n",
            "Train size( Tomato___healthy ):  954\n",
            "Validation size( Tomato___healthy ):  318\n",
            "Test size( Tomato___healthy ):  318\n",
            "Train size( Tomato___Late_blight ):  1145\n",
            "Validation size( Tomato___Late_blight ):  381\n",
            "Test size( Tomato___Late_blight ):  381\n",
            "Train size( Tomato___Leaf_Mold ):  571\n",
            "Validation size( Tomato___Leaf_Mold ):  190\n",
            "Test size( Tomato___Leaf_Mold ):  190\n",
            "Train size( Tomato___Septoria_leaf_spot ):  1062\n",
            "Validation size( Tomato___Septoria_leaf_spot ):  354\n",
            "Test size( Tomato___Septoria_leaf_spot ):  354\n",
            "Train size( Tomato___Spider_mites Two-spotted_spider_mite ):  1005\n",
            "Validation size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
            "Test size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
            "Train size( Tomato___Target_Spot ):  842\n",
            "Validation size( Tomato___Target_Spot ):  280\n",
            "Test size( Tomato___Target_Spot ):  280\n",
            "Train size( Tomato___Tomato_mosaic_virus ):  223\n",
            "Validation size( Tomato___Tomato_mosaic_virus ):  74\n",
            "Test size( Tomato___Tomato_mosaic_virus ):  74\n",
            "Train size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  3214\n",
            "Validation size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n",
            "Test size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        " \n",
        "for cls in classes_list:\n",
        "    path = os.path.join(original_dataset_dir, cls)\n",
        "    fnames = os.listdir(path)\n",
        " \n",
        "    train_size = math.floor(len(fnames) * 0.6)\n",
        "    validation_size = math.floor(len(fnames) * 0.2)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "    \n",
        "    train_fnames = fnames[:train_size]\n",
        "    print(\"Train size(\",cls,\"): \", len(train_fnames))\n",
        "    for fname in train_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(train_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    print(\"Validation size(\",cls,\"): \", len(validation_fnames))\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    test_fnames = fnames[(train_size+validation_size):(validation_size + train_size +test_size)]\n",
        "\n",
        "    print(\"Test size(\",cls,\"): \", len(test_fnames))\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCmL1NkDvEDS"
      },
      "source": [
        "## 베이스라인 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6WCF-1tvEDT"
      },
      "source": [
        "* 베이스라인 모델 학습을 위한 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjNXkSltvEDU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        " \n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "BATCH_SIZE = 256 \n",
        "EPOCH = 30 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYsACiRfvEDV"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder \n",
        " \n",
        "transform_base = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()]) \n",
        "train_dataset = ImageFolder(root='./splitted/train', transform=transform_base) \n",
        "val_dataset = ImageFolder(root='./splitted/val', transform=transform_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAVvS4YRvEDW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOG816k9vEDX"
      },
      "source": [
        "* 베이스라인 모델 설계하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOlYre5tvEDX"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        " \n",
        "class Net(nn.Module): \n",
        "  \n",
        "    def __init__(self): \n",
        "    \n",
        "        super(Net, self).__init__() \n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) \n",
        "        self.pool = nn.MaxPool2d(2,2)  \n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  \n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)  \n",
        "\n",
        "        self.fc1 = nn.Linear(4096, 512) \n",
        "        self.fc2 = nn.Linear(512, 33) \n",
        "    \n",
        "    def forward(self, x):  \n",
        "    \n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)  \n",
        "        x = self.pool(x) \n",
        "        x = F.dropout(x, p=0.25, training=self.training) \n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x) \n",
        "        x = self.pool(x) \n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = self.conv3(x) \n",
        "        x = F.relu(x) \n",
        "        x = self.pool(x) \n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = x.view(-1, 4096)  \n",
        "        x = self.fc1(x) \n",
        "        x = F.relu(x) \n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.fc2(x) \n",
        "\n",
        "        return F.log_softmax(x, dim=1)  \n",
        "\n",
        "model_base = Net().to(DEVICE)  \n",
        "optimizer = optim.Adam(model_base.parameters(), lr=0.001) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I65t1IrivEDY"
      },
      "source": [
        "* 모델 학습을 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t_Hk1nWvEDY"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()  \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE) \n",
        "        optimizer.zero_grad() \n",
        "        output = model(data)  \n",
        "        loss = F.cross_entropy(output, target) \n",
        "        loss.backward()  \n",
        "        optimizer.step()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHQg7Y5hvEDZ"
      },
      "source": [
        "* 모델 평가를 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P-4yGp1vEDa"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()  \n",
        "    test_loss = 0 \n",
        "    correct = 0   \n",
        "    \n",
        "    with torch.no_grad(): \n",
        "        for data, target in test_loader:  \n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)  \n",
        "            output = model(data) \n",
        "            \n",
        "            test_loss += F.cross_entropy(output,target, reduction='sum').item() \n",
        " \n",
        "            \n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() \n",
        "   \n",
        "    test_loss /= len(test_loader.dataset) \n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) \n",
        "    return test_loss, test_accuracy  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAdmpf7IvEDa"
      },
      "source": [
        "* 모델 학습을 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo1urbeovEDb",
        "outputId": "e2d8a143-57dd-4888-bb5a-a50cf7f642dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- epoch 1 ----------------\n",
            "train Loss: 1.4979, Accuracy: 56.62%\n",
            "val Loss: 1.5244, Accuracy: 56.52%\n",
            "Completed in 0m 40s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 1.1273, Accuracy: 65.63%\n",
            "val Loss: 1.1712, Accuracy: 64.20%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 0.7655, Accuracy: 76.57%\n",
            "val Loss: 0.8181, Accuracy: 74.93%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 0.5791, Accuracy: 82.15%\n",
            "val Loss: 0.6426, Accuracy: 80.26%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 5 ----------------\n",
            "train Loss: 0.4588, Accuracy: 85.52%\n",
            "val Loss: 0.5320, Accuracy: 83.35%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 6 ----------------\n",
            "train Loss: 0.4259, Accuracy: 86.94%\n",
            "val Loss: 0.5050, Accuracy: 84.02%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 7 ----------------\n",
            "train Loss: 0.4291, Accuracy: 86.25%\n",
            "val Loss: 0.5187, Accuracy: 83.48%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 8 ----------------\n",
            "train Loss: 0.2987, Accuracy: 91.13%\n",
            "val Loss: 0.3911, Accuracy: 87.63%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 9 ----------------\n",
            "train Loss: 0.2998, Accuracy: 90.61%\n",
            "val Loss: 0.3994, Accuracy: 87.02%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 10 ----------------\n",
            "train Loss: 0.2605, Accuracy: 92.31%\n",
            "val Loss: 0.3652, Accuracy: 88.57%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 11 ----------------\n",
            "train Loss: 0.2140, Accuracy: 93.69%\n",
            "val Loss: 0.3203, Accuracy: 89.72%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 12 ----------------\n",
            "train Loss: 0.1875, Accuracy: 94.56%\n",
            "val Loss: 0.2981, Accuracy: 90.54%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 13 ----------------\n",
            "train Loss: 0.2198, Accuracy: 93.25%\n",
            "val Loss: 0.3411, Accuracy: 88.90%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 14 ----------------\n",
            "train Loss: 0.1760, Accuracy: 94.74%\n",
            "val Loss: 0.2992, Accuracy: 90.64%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 15 ----------------\n",
            "train Loss: 0.1488, Accuracy: 95.89%\n",
            "val Loss: 0.2698, Accuracy: 91.44%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 16 ----------------\n",
            "train Loss: 0.1535, Accuracy: 95.77%\n",
            "val Loss: 0.2807, Accuracy: 90.99%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 17 ----------------\n",
            "train Loss: 0.1341, Accuracy: 96.23%\n",
            "val Loss: 0.2631, Accuracy: 91.48%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 18 ----------------\n",
            "train Loss: 0.1252, Accuracy: 96.21%\n",
            "val Loss: 0.2620, Accuracy: 91.41%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 19 ----------------\n",
            "train Loss: 0.1189, Accuracy: 96.59%\n",
            "val Loss: 0.2673, Accuracy: 91.53%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 20 ----------------\n",
            "train Loss: 0.1215, Accuracy: 96.48%\n",
            "val Loss: 0.2678, Accuracy: 91.30%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 21 ----------------\n",
            "train Loss: 0.1034, Accuracy: 97.15%\n",
            "val Loss: 0.2454, Accuracy: 91.94%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 22 ----------------\n",
            "train Loss: 0.1044, Accuracy: 96.89%\n",
            "val Loss: 0.2545, Accuracy: 91.89%\n",
            "Completed in 0m 38s\n",
            "-------------- epoch 23 ----------------\n",
            "train Loss: 0.0905, Accuracy: 97.61%\n",
            "val Loss: 0.2444, Accuracy: 92.26%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 24 ----------------\n",
            "train Loss: 0.0803, Accuracy: 98.09%\n",
            "val Loss: 0.2394, Accuracy: 92.50%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 25 ----------------\n",
            "train Loss: 0.0947, Accuracy: 97.11%\n",
            "val Loss: 0.2602, Accuracy: 91.53%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 26 ----------------\n",
            "train Loss: 0.0658, Accuracy: 98.41%\n",
            "val Loss: 0.2205, Accuracy: 93.05%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 27 ----------------\n",
            "train Loss: 0.0551, Accuracy: 98.78%\n",
            "val Loss: 0.2066, Accuracy: 93.37%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 28 ----------------\n",
            "train Loss: 0.0586, Accuracy: 98.65%\n",
            "val Loss: 0.2121, Accuracy: 92.84%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 29 ----------------\n",
            "train Loss: 0.0582, Accuracy: 98.66%\n",
            "val Loss: 0.2132, Accuracy: 93.17%\n",
            "Completed in 0m 37s\n",
            "-------------- epoch 30 ----------------\n",
            "train Loss: 0.0519, Accuracy: 98.75%\n",
            "val Loss: 0.2196, Accuracy: 93.09%\n",
            "Completed in 0m 37s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import copy\n",
        " \n",
        "def train_baseline(model ,train_loader, val_loader, optimizer, num_epochs = 30):\n",
        "    best_acc = 0.0  \n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) \n",
        " \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        since = time.time()  \n",
        "        train(model, train_loader, optimizer)\n",
        "        train_loss, train_acc = evaluate(model, train_loader) \n",
        "        val_loss, val_acc = evaluate(model, val_loader)\n",
        "        \n",
        "        if val_acc > best_acc: \n",
        "            best_acc = val_acc \n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        time_elapsed = time.time() - since \n",
        "        print('-------------- epoch {} ----------------'.format(epoch))\n",
        "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))   \n",
        "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) \n",
        "    model.load_state_dict(best_model_wts)  \n",
        "    return model\n",
        " \n",
        "\n",
        "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)  \t #(16)\n",
        "torch.save(base,'baseline.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyWed36kvEDc"
      },
      "source": [
        "## Transfer Learning 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SLh5SNBvEDc"
      },
      "source": [
        "* Transfer Learning을 위한 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1rGUW7WvEDd"
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([transforms.Resize([64,64]), \n",
        "        transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(),  \n",
        "        transforms.RandomCrop(52), transforms.ToTensor(), \n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]),\n",
        "    \n",
        "    'val': transforms.Compose([transforms.Resize([64,64]),  \n",
        "        transforms.RandomCrop(52), transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEZdkaaJvEDd"
      },
      "outputs": [],
      "source": [
        "data_dir = './splitted' \n",
        "image_datasets = {x: ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train', 'val']} \n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']} \n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "class_names = image_datasets['train'].classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYZMASiMvEDe"
      },
      "source": [
        "* Pre-Trained Model 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6KcFHfuvEDe",
        "outputId": "85b7bc4f-a0a8-40a2-f2a5-b16971f8b6b6",
        "colab": {
          "referenced_widgets": [
            "1d43af081ac541c5b0d78e5cddff7ac0"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\USER/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d43af081ac541c5b0d78e5cddff7ac0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torchvision import models\n",
        " \n",
        "resnet = models.resnet50(pretrained=True)  \n",
        "num_ftrs = resnet.fc.in_features   \n",
        "resnet.fc = nn.Linear(num_ftrs, 33) \n",
        "resnet = resnet.to(DEVICE)\n",
        " \n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)\n",
        " \n",
        "from torch.optim import lr_scheduler\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcg9aGmwvEDe"
      },
      "source": [
        "* Pre-Trained Model의 일부 Layer Freeze하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGH6rYDrvEDe"
      },
      "outputs": [],
      "source": [
        "ct = 0 \n",
        "for child in resnet.children():  \n",
        "    ct += 1  \n",
        "    if ct < 6: \n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_BY_iukvEDf"
      },
      "source": [
        "* Transfer Learning 모델 학습과 검증을 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkguId1vvEDf"
      },
      "outputs": [],
      "source": [
        "def train_resnet(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())  \n",
        "    best_acc = 0.0  \n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
        "        since = time.time()                                     \n",
        "        for phase in ['train', 'val']: \n",
        "            if phase == 'train': \n",
        "                model.train() \n",
        "            else:\n",
        "                model.eval()     \n",
        " \n",
        "            running_loss = 0.0  \n",
        "            running_corrects = 0  \n",
        " \n",
        "            \n",
        "            for inputs, labels in dataloaders[phase]: \n",
        "                inputs = inputs.to(DEVICE)  \n",
        "                labels = labels.to(DEVICE)  \n",
        "                \n",
        "                optimizer.zero_grad() \n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'):  \n",
        "                    outputs = model(inputs)  \n",
        "                    _, preds = torch.max(outputs, 1) \n",
        "                    loss = criterion(outputs, labels)  \n",
        "    \n",
        "                    if phase == 'train':   \n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        " \n",
        "                running_loss += loss.item() * inputs.size(0)  \n",
        "                running_corrects += torch.sum(preds == labels.data)  \n",
        "            if phase == 'train':  \n",
        "                scheduler.step()\n",
        " \n",
        "            epoch_loss = running_loss/dataset_sizes[phase]  \n",
        "            epoch_acc = running_corrects.double()/dataset_sizes[phase]  \n",
        " \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) \n",
        " \n",
        "          \n",
        "            if phase == 'val' and epoch_acc > best_acc: \n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        " \n",
        "        time_elapsed = time.time() - since  \n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        " \n",
        "    model.load_state_dict(best_model_wts) \n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXuKoCT2vEDg"
      },
      "source": [
        "* 모델 학습을 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFKC4qjEvEDg",
        "outputId": "18c71cb1-706c-4e4d-ccf4-113b0b182404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- epoch 1 ----------------\n",
            "train Loss: 0.5719 Acc: 0.8268\n",
            "val Loss: 0.2983 Acc: 0.9106\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 0.2210 Acc: 0.9294\n",
            "val Loss: 0.2508 Acc: 0.9254\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 0.1710 Acc: 0.9448\n",
            "val Loss: 0.2152 Acc: 0.9337\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 0.1342 Acc: 0.9575\n",
            "val Loss: 0.1449 Acc: 0.9507\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 5 ----------------\n",
            "train Loss: 0.1074 Acc: 0.9642\n",
            "val Loss: 0.1185 Acc: 0.9624\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 6 ----------------\n",
            "train Loss: 0.1041 Acc: 0.9663\n",
            "val Loss: 0.1206 Acc: 0.9603\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 7 ----------------\n",
            "train Loss: 0.0887 Acc: 0.9715\n",
            "val Loss: 0.1254 Acc: 0.9618\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 8 ----------------\n",
            "train Loss: 0.0539 Acc: 0.9823\n",
            "val Loss: 0.0471 Acc: 0.9847\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 9 ----------------\n",
            "train Loss: 0.0293 Acc: 0.9902\n",
            "val Loss: 0.0428 Acc: 0.9859\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 10 ----------------\n",
            "train Loss: 0.0257 Acc: 0.9920\n",
            "val Loss: 0.0408 Acc: 0.9874\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 11 ----------------\n",
            "train Loss: 0.0224 Acc: 0.9926\n",
            "val Loss: 0.0357 Acc: 0.9884\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 12 ----------------\n",
            "train Loss: 0.0204 Acc: 0.9930\n",
            "val Loss: 0.0359 Acc: 0.9881\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 13 ----------------\n",
            "train Loss: 0.0179 Acc: 0.9940\n",
            "val Loss: 0.0345 Acc: 0.9895\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 14 ----------------\n",
            "train Loss: 0.0162 Acc: 0.9950\n",
            "val Loss: 0.0314 Acc: 0.9891\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 15 ----------------\n",
            "train Loss: 0.0146 Acc: 0.9960\n",
            "val Loss: 0.0311 Acc: 0.9892\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 16 ----------------\n",
            "train Loss: 0.0140 Acc: 0.9955\n",
            "val Loss: 0.0279 Acc: 0.9910\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 17 ----------------\n",
            "train Loss: 0.0146 Acc: 0.9954\n",
            "val Loss: 0.0319 Acc: 0.9899\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 18 ----------------\n",
            "train Loss: 0.0132 Acc: 0.9962\n",
            "val Loss: 0.0329 Acc: 0.9902\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 19 ----------------\n",
            "train Loss: 0.0129 Acc: 0.9959\n",
            "val Loss: 0.0290 Acc: 0.9905\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 20 ----------------\n",
            "train Loss: 0.0148 Acc: 0.9947\n",
            "val Loss: 0.0299 Acc: 0.9901\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 21 ----------------\n",
            "train Loss: 0.0133 Acc: 0.9958\n",
            "val Loss: 0.0290 Acc: 0.9899\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 22 ----------------\n",
            "train Loss: 0.0131 Acc: 0.9956\n",
            "val Loss: 0.0305 Acc: 0.9894\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 23 ----------------\n",
            "train Loss: 0.0133 Acc: 0.9956\n",
            "val Loss: 0.0286 Acc: 0.9905\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 24 ----------------\n",
            "train Loss: 0.0121 Acc: 0.9964\n",
            "val Loss: 0.0317 Acc: 0.9897\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 25 ----------------\n",
            "train Loss: 0.0121 Acc: 0.9964\n",
            "val Loss: 0.0296 Acc: 0.9905\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 26 ----------------\n",
            "train Loss: 0.0137 Acc: 0.9957\n",
            "val Loss: 0.0311 Acc: 0.9892\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 27 ----------------\n",
            "train Loss: 0.0129 Acc: 0.9957\n",
            "val Loss: 0.0304 Acc: 0.9905\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 28 ----------------\n",
            "train Loss: 0.0126 Acc: 0.9958\n",
            "val Loss: 0.0301 Acc: 0.9910\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 29 ----------------\n",
            "train Loss: 0.0129 Acc: 0.9961\n",
            "val Loss: 0.0302 Acc: 0.9906\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 30 ----------------\n",
            "train Loss: 0.0128 Acc: 0.9962\n",
            "val Loss: 0.0295 Acc: 0.9904\n",
            "Completed in 0m 24s\n",
            "Best val Acc: 0.990988\n"
          ]
        }
      ],
      "source": [
        "model_resnet50 = train_resnet(resnet, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH) \n",
        "\n",
        "torch.save(model_resnet50, 'resnet50.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoIWKrfWvEDh"
      },
      "source": [
        "## 모델 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2gapGbwvEDh"
      },
      "source": [
        "* 베이스라인 모델 평가를 위한 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJf4FbgxvEDm"
      },
      "outputs": [],
      "source": [
        "transform_base = transforms.Compose([transforms.Resize([64,64]),transforms.ToTensor()])\n",
        "test_base = ImageFolder(root='./splitted/test',transform=transform_base)  \n",
        "test_loader_base = torch.utils.data.DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IpQegkKvEDn"
      },
      "source": [
        "* Transfer Learning모델 평가를 위한 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t37T4ElovEDn"
      },
      "outputs": [],
      "source": [
        "transform_resNet = transforms.Compose([\n",
        "        transforms.Resize([64,64]),  \n",
        "        transforms.RandomCrop(52),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "    ])\n",
        "    \n",
        "test_resNet = ImageFolder(root='./splitted/test', transform=transform_resNet) \n",
        "test_loader_resNet = torch.utils.data.DataLoader(test_resNet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ox9NFZBvEDq"
      },
      "source": [
        "* 베이스라인 모델 성능 평가하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pNgLu8mvEDq",
        "outputId": "0b72673e-0ff7-4b86-daf8-be4db2dbc2ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "baseline test acc:   93.07798222556015\n"
          ]
        }
      ],
      "source": [
        "baseline=torch.load('baseline.pt') \n",
        "baseline.eval()  \n",
        "test_loss, test_accuracy = evaluate(baseline, test_loader_base)\n",
        "\n",
        "print('baseline test acc:  ', test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDkXcLjAvEDr"
      },
      "source": [
        "* Transfer Learning 모델 성능 평가하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26ttIJzxvEDr",
        "outputId": "7bbd3439-f447-4726-ef57-0a5d112eb477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet test acc:   98.96107147327575\n"
          ]
        }
      ],
      "source": [
        "resnet50=torch.load('resnet50.pt') \n",
        "resnet50.eval()  \n",
        "test_loss, test_accuracy = evaluate(resnet50, test_loader_resNet)\n",
        "\n",
        "print('ResNet test acc:  ', test_accuracy)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "PART_04_Plant_Leaf_Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
